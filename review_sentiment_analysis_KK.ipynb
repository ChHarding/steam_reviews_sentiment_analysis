{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which game to scrape\n",
    "#appid = 24780  # simcity4\n",
    "appid = 2357570 # over watch 2\n",
    "\n",
    "# set base name for the file\n",
    "#base_name = 'simcity4'\n",
    "base_name = 'overwatch2'\n",
    "\n",
    "scrape_file = f'reviews_{base_name}.csv'\n",
    "results_file = f'results_{base_name}.csv'\n",
    "extended_file = f'reviews_{base_name}_extended.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file to write the scraped data\n",
    "with open(scrape_file, \"w+\", encoding='utf-8') as fo:\n",
    "    # Write the header for the CSV file\n",
    "    print(\"review,author_vote,other_votes,weighted_vote_score,votes_funny,comment_count,timestamp_created,playtime_at_review\", file=fo)\n",
    "\n",
    "    cursor = '*'  # Start with the initial cursor\n",
    "    base_url = f'https://store.steampowered.com/appreviews/{appid}'\n",
    "\n",
    "    curr_page = 0 # Initialize the current page counter\n",
    "    while True:\n",
    "        # Set the parameters for the API request\n",
    "        params = { # https://partner.steamgames.com/doc/store/getreviews\n",
    "            'json' : 1,\n",
    "            'filter' : 'all', # sort by: recent, updated, all (helpfulness)\n",
    "            'language' : 'english', # https://partner.steamgames.com/doc/store/localization # Only fetch reviews in English\n",
    "            'day_range' : 9223372036854775807, # shows reviews from all time \n",
    "            'review_type' : 'all', # include all reviews (positive and negative)\n",
    "            'purchase_type' : 'all', # all, non_steam_purchase, steam\n",
    "            'num_per_page' : 100, # Number of reviews per page\n",
    "            'cursor': cursor,  # Use the cursor returned from the last request\n",
    "        }\n",
    "        response = requests.get(base_url, params=params) #Make the API request\n",
    "        data = response.json() # Parse the JSON response\n",
    "\n",
    "        print(curr_page, end=\" \") # Print the current page number\n",
    "        curr_page += 1 # Increment the current page counter\n",
    "\n",
    "        # Break the loop if there are no more reviews\n",
    "        if 'reviews' in data and not data['reviews']:\n",
    "            break\n",
    "        \n",
    "        page_list = data['reviews'] # Get the list of reviews from the response\n",
    "\n",
    "        for i, page in enumerate(page_list):\n",
    "            words =  page[\"review\"].split() # Split the review text into words\n",
    "\n",
    "            if len(words) < 30: \n",
    "                continue # Skip reviews with less than 30 words\n",
    "            \n",
    "            # Prepare the review text for CSV\n",
    "            rev = '\"' + page[\"review\"].replace('\"',\"\") + '\"'\n",
    "            author_vote = 1 if page['voted_up'] else 0 # Convert voted_up to binary\n",
    "            other_votes = page['votes_up']\n",
    "            weighted_vote_score = page[\"weighted_vote_score\"] \n",
    "            votes_funny = page[\"votes_funny\"] # Get votes_funny or default to 0\n",
    "            comment_count = page[\"comment_count\"] # Get comment_count or default to 0\n",
    "            timestamp_created = datetime.datetime.fromtimestamp(page[\"timestamp_created\"]).strftime('%Y-%m-%d %H:%M:%S') # Convert timestamp to human-readable format\n",
    "            playtime_at_review = page[\"author\"].get(\"playtime_at_review\", 0) #Get playtime_at_review or default to 0\n",
    "\n",
    "            # Write the extracted data to the CSV file\n",
    "            print(rev, ',', author_vote, ',', other_votes, ',', weighted_vote_score, ',', votes_funny, ',', comment_count, ',', timestamp_created, ',', playtime_at_review, file=fo)\n",
    "\n",
    "        # Update the cursor for the next API request\n",
    "        cursor = data['cursor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the SentiWordNet lexicon\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import pandas as pd\n",
    "\n",
    "# Download the SentiWordNet and WordNet resources\n",
    "nltk.download('sentiwordnet');\n",
    "nltk.download('wordnet');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word, pos):\n",
    "    synsets = list(swn.senti_synsets(word, pos))\n",
    "    if not synsets:\n",
    "        return 0, 0, 0\n",
    "    synset = synsets[0]\n",
    "    return synset.pos_score(), synset.neg_score(), synset.obj_score()\n",
    "\n",
    "def sentiment_analysis_SentiWordNet(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    sentiment_scores = {'positive': 0, 'negative': 0, 'objective': 0 }\n",
    "    word_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    noun_count = 0\n",
    "    adv_count = 0\n",
    "\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "\n",
    "        if wn_tag is not None:\n",
    "            #print(f\"{word}={wn_tag}\", end=\", \")\n",
    "            pos_score, neg_score, obj_score = get_sentiment(word, wn_tag)\n",
    "            sentiment_scores['positive'] += pos_score\n",
    "            sentiment_scores['negative'] += neg_score\n",
    "            sentiment_scores['objective'] += obj_score\n",
    "            word_count += 1\n",
    "            # CH: you need to only count for adjectives, verbs, nouns, and adverbs\n",
    "            # if wn_tag is of that type !\n",
    "            if wn_tag == wn.ADJ:\n",
    "                adj_count += 1\n",
    "            if wn_tag == wn.VERB:\n",
    "                verb_count += 1 \n",
    "            if wn_tag == wn.NOUN:\n",
    "                noun_count += 1\n",
    "            if wn_tag == wn.ADV:\n",
    "                adv_count += 1\n",
    "\n",
    "    # Normalize the score by the number of words\n",
    "    if word_count > 0:\n",
    "        sentiment_scores['positive'] /= word_count\n",
    "        sentiment_scores['negative'] /= word_count\n",
    "        sentiment_scores['objective'] /= word_count\n",
    "    return sentiment_scores['positive'], sentiment_scores['negative'], sentiment_scores['objective'], word_count, adj_count, verb_count, noun_count, adv_count\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scraped CSV file\n",
    "df = pd.read_csv(scrape_file, encoding='utf-8'  )\n",
    "print(len(df), \"reviews loaded\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on each review\n",
    "# if there is already a results file, skip this and create\n",
    "# an extended file in the next cell\n",
    "results = []\n",
    "\n",
    "reviews = df['review'].tolist()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    pos_score, neg_score, obj_score, word_count, adj_count, verb_count, noun_count, adv_count = sentiment_analysis_SentiWordNet(review)\n",
    "    results.append({\n",
    "        #'review': review, # CH no need for the reviews after this stage\n",
    "        'positive_score': round(pos_score, 3),\n",
    "        'negative_score': round(neg_score, 3),\n",
    "        'objective_score': round(obj_score, 3),\n",
    "        'word_count': word_count,\n",
    "        'adj_count': adj_count,\n",
    "        'verb_count': verb_count,\n",
    "        'noun_count': noun_count,\n",
    "        'adv_count': adv_count,\n",
    "    })\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=\" \")\n",
    "    \n",
    "# Convert the results to a DataFrame for better readability\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file incase the concat does not work\n",
    "results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform descriptive statistics, using the describe() method from pandas to compute descriptive statistics (mean, standard deviation, min, max, etc.) for the sentiment scores and word count.\n",
    "#descriptive_stats = results_df.describe()\n",
    "#print(descriptive_stats)\n",
    "\n",
    "# Perform descriptive statistics on the combined DataFrame\n",
    "#descriptive_stats = combined_df.describe(include='all')\n",
    "#print(descriptive_stats)\n",
    "\n",
    "# read in file so it's independent from the previous cells\n",
    "results_df = pd.read_csv(results_file)\n",
    "\n",
    "# Combine the original DataFrame with the sentiment analysis results\n",
    "combined_df = pd.concat([df, results_df], axis=1)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "# print(combined_df)\n",
    "\n",
    "# Perform descriptive statistics on the combined DataFrame\n",
    "# descriptive_stats = combined_df.describe(include='all')\n",
    "# print(descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write combined data to a new file\n",
    "# this is so we can load the data back in later without having to re-compute the sentiment analysis\n",
    "combined_df.to_csv(extended_file, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file'\n",
    "df = pd.read_csv(extended_file)\n",
    "\n",
    "# make sure we don't have duplicate reviews so drop rows where positive_score, negative_score, objective_score \n",
    "# are the same\n",
    "df = df.drop_duplicates(subset=['positive_score', 'negative_score', 'objective_score'])\n",
    "\n",
    "# re-index the dataframe\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(len(df), \"unique reviews loaded\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics of numeric data in the dataset\n",
    "# CH note that all reviews now are listed as unique i.e. no duplicates\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH print out the timespan of the reviews\n",
    "# this is useful for dweciding what timespand to aggregate to\n",
    "print(df['timestamp_created'].min(), df['timestamp_created'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH aggregate means of values over time (monthly)\n",
    "df_agg  = df.copy() \n",
    "df_agg['timestamp_created'] = pd.to_datetime(df_agg['timestamp_created']) # make python datetime object\n",
    "df_agg.set_index('timestamp_created', inplace=True) # set index to timestamp_created\n",
    "\n",
    "# drop review column as it is not numeric\n",
    "df_agg.drop(columns=['review'], inplace=True)\n",
    "\n",
    "# resample all numeric columns to monthly\n",
    "df_agg = df_agg.resample('MS').mean() # resample all numeric columns to monthly\n",
    "\n",
    "# resample all numeric columns to 3 month\n",
    "#df_agg = df_agg.resample('3MS').mean() # resample all numeric columns to 6 month\n",
    "\n",
    "# resample all numeric columns to 6 month\n",
    "#df_agg = df_agg.resample('6MS').mean() # resample all numeric columns to 6 month\n",
    "\n",
    "df_agg.reset_index(inplace=True) # reset index to make timestamp_created a column again\n",
    "df_agg['timestamp_created'] = df_agg['timestamp_created'].dt.strftime('%Y-%m') # format timestamp_created\n",
    "display(df_agg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_over_time(columns, df, logy=False):\n",
    "    df = df[columns + ['timestamp_created']]\n",
    "    # Melting the DataFrame\n",
    "    df_melted = df.melt('timestamp_created', var_name='variable', value_name='value')\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    sns.lineplot(data=df_melted, x='timestamp_created', y='value', hue='variable')\n",
    "    if logy:\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_time(['positive_score', 'negative_score', 'objective_score'], df_agg, logy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_time(['word_count', 'adj_count', 'verb_count', 'noun_count', 'adv_count'], df_agg, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_time(['author_vote', 'other_votes', 'votes_funny', 'comment_count'], df_agg, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Numerical Variables\n",
    "# Visualize the distribution of numerical variables using histograms and boxplots.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram for numerical columns\n",
    "\n",
    "dfs = df[['positive_score', 'negative_score', 'objective_score']]\n",
    "dfs.hist(bins=100, figsize=(20, 10))\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for numerical columns\n",
    "#plt.figure(figsize=(15, 10))\n",
    "#sns.boxplot(data=df)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df[['word_count', 'adj_count', 'verb_count', 'noun_count', 'adv_count']]\n",
    "axes = dfc.hist(bins=100, figsize=(20, 10))\n",
    "axes.set_xlim([0, 500])  # Limit the x-axis to 500\n",
    "# set all x-axis to log scale\n",
    "#for ax in axes.flatten():\n",
    "#    ax.set_xscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns to df that describe the ratio of adj, verb, noun, adv to word count\n",
    "df['adj_ratio'] = df['adj_count'] / df['word_count']\n",
    "df['verb_ratio'] = df['verb_count'] / df['word_count']\n",
    "df['noun_ratio'] = df['noun_count'] / df['word_count']\n",
    "df['adv_ratio'] = df['adv_count'] / df['word_count']\n",
    "\n",
    "dfr = df[['adj_ratio', 'verb_ratio', 'noun_ratio', 'adv_ratio']]\n",
    "axes = dfr.hist(bins=100, figsize=(20, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to df add a column that indicates which of adj, verb, noun, adv is the biggest \n",
    "# encode with \"adj_count\", \"verb_count\", \"noun_count\", \"adv_count\"\n",
    "df['POS_ratio'] = df[['adj_ratio', 'verb_ratio', 'noun_ratio', 'adv_ratio']].idxmax(axis=1)\n",
    "\n",
    "# in df['POS_ratio'] rename adj_ratio to adj_most, verb_ratio to verb_most, noun_ratio to noun_most, adv_ratio to adv_most\n",
    "df['POS_ratio'] = df['POS_ratio'].replace({'adj_ratio': 'adj_most', 'verb_ratio': 'verb_most', 'noun_ratio': 'noun_most', 'adv_ratio': 'adv_most'})\n",
    "\n",
    "# show the distribution of the POS_ratio\n",
    "df['POS_ratio'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting KDE plots\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# KDE plot for positive sentiment score vs adjective count\n",
    "sns.kdeplot(data=results_df, x='word_count', y='positive_score', fill=True, cmap=\"Blues\", thresh=0.05)\n",
    "plt.title('KDE Plot: Adjective Count vs Positive Sentiment Score')\n",
    "plt.xlabel('Adjective Count')\n",
    "plt.ylabel('Positive Sentiment Score')\n",
    "plt.xlim(0, 600) # limit x-axis to 0-600\n",
    "plt.ylim(0, 0.2)\n",
    "plt.show()\n",
    "\n",
    "# KDE plot for negative sentiment score vs adjective count\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.kdeplot(data=results_df, x='word_count', y='negative_score', fill=True, cmap=\"Reds\", thresh=0.05)\n",
    "plt.title('KDE Plot: Adjective Count vs Negative Sentiment Score')\n",
    "plt.xlabel('Adjective Count')\n",
    "plt.ylabel('Negative Sentiment Score')\n",
    "plt.xlim(0, 600)\n",
    "plt.ylim(0, 0.2)\n",
    "plt.show()\n",
    "\n",
    "# KDE plot for objective sentiment score vs adjective count\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.kdeplot(data=results_df, x='word_count', y='objective_score', fill=True, cmap=\"Greens\", thresh=0.05)\n",
    "plt.title('KDE Plot: Adjective Count vs Objective Sentiment Score')\n",
    "plt.xlabel('Adjective Count')\n",
    "plt.ylabel('Objective Sentiment Score')\n",
    "plt.xlim(0, 700)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file'\n",
    "df = pd.read_csv(extended_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Based on the 574 Lecture 41-Data Science\n",
    "# scatterplot of votes_funny vs positive_score\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(data=results.df, x=\"votes_funny\", y=\"positive_score\", ax=ax)\n",
    "ax.set(title='Votes_funny vs positive_score', xlabel='Votes_funny', ylabel='Positive_score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the 574 Lecture 41-Data Science\n",
    "# scatterplot of votes_funny vs positive_score with regression line\n",
    "ax1, fig =  plt.subplots(figsize= (10, 10))\n",
    "plt.close(plt.gcf()) # I'm unclear why this is needed here but whatevs\n",
    "\n",
    "ax1 = sns.lmplot(data=results.df, x=\"votes_funny\", y=\"positive_score\", height=10, aspect=1,)\n",
    "ax1.set(title='Votes_funny vs positive_score', xlabel='Votes_funny', ylabel='Positive_score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient\n",
    "correlation = df['votes_funny'].corr(df['positive_score'])\n",
    "print(f'Correlation coefficient between votes_funny and positive_score: {correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from 574 Lecture 41-Data Science\n",
    "# visualizing the correlation coefficient\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "r2 = numeric_df.corr(method='pearson')   # pearson is the standard method of calculation the goodness of fit\n",
    "#r2 = df.corr(method='spearman') # performs a rank-ordering first\n",
    "display(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step: correlation matrix between all values. However, first I need to get the R2s.\n",
    "ax1, fig =  plt.subplots(figsize= (13, 13))\n",
    "sns.heatmap(abs(r2), \n",
    "            annot = True, \n",
    "            fmt=\".1f\"); # show numbers, but with 1 digit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
