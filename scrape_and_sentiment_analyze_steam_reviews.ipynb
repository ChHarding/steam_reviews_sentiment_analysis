{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests pandas seaborn matplotlib plotly nbformat --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # pip install requests    \n",
    "import pandas as pd # pip install pandas\n",
    "import seaborn as sns # pip install seaborn\n",
    "import matplotlib.pyplot as plt # pip install matplotlib\n",
    "import plotly.express as px  # pip install plotly\n",
    "\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import time \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which game to scrape\n",
    "appid = << your game's app id >>   # will be in the steam store URL for your game, like this: store.steampowered.com/appreviews/<<appid>>\n",
    "\n",
    "# Examples of app ids:\n",
    "#appid = 24780  # simcity4\n",
    "#appid = 2357570 # over watch 2\n",
    "#appid = 413150 # stardew valley\n",
    "\n",
    "# set base name for the file\n",
    "base_name = << your games name >> #\n",
    "\n",
    "scrape_file = f'reviews_{base_name}.csv'\n",
    "results_file = f'results_{base_name}.csv'\n",
    "extended_file = f'reviews_{base_name}_extended.csv'\n",
    "\n",
    "# make a new folder for the base name if it does not exist\n",
    "if not os.path.exists(base_name):\n",
    "    os.makedirs(base_name)\n",
    "\n",
    "# prepend the folder name to all file names\n",
    "scrape_file = f'{base_name}/{scrape_file}'\n",
    "results_file = f'{base_name}/{results_file}'\n",
    "extended_file = f'{base_name}/{extended_file}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Steam reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file to write the scraped data and write them to a CSV file\n",
    "with open(scrape_file, \"w+\", encoding='utf-8') as fo:\n",
    "    # Write the header for the CSV file\n",
    "    print(\"review,author_vote,other_votes,weighted_vote_score,votes_funny,comment_count,timestamp_created,playtime_at_review\", file=fo)\n",
    "\n",
    "    cursor = '*'  # Start with the initial cursor\n",
    "    base_url = f'https://store.steampowered.com/appreviews/{appid}'\n",
    "\n",
    "    curr_page = 0 # Initialize the current page counter\n",
    "    while True:\n",
    "        # Set the parameters for the API request\n",
    "        params = { # https://partner.steamgames.com/doc/store/getreviews\n",
    "            'json' : 1,\n",
    "            'filter' : 'all', # sort by: recent, updated, all (helpfulness)\n",
    "            'language' : 'english', # https://partner.steamgames.com/doc/store/localization # Only fetch reviews in English\n",
    "            'day_range' : 9223372036854775807, # shows reviews from all time \n",
    "            'review_type' : 'all', # include all reviews (positive and negative)\n",
    "            'purchase_type' : 'all', # all, non_steam_purchase, steam\n",
    "            'num_per_page' : 100, # Number of reviews per page\n",
    "            'cursor': cursor,  # Use the cursor returned from the last request\n",
    "        }\n",
    "        response = requests.get(base_url, params=params) #Make the API request\n",
    "        data = response.json() # Parse the JSON response\n",
    "\n",
    "        print(curr_page, end=\" \") # Print the current page number\n",
    "        curr_page += 1 # Increment the current page counter\n",
    "\n",
    "        # Break the loop if there are no more reviews\n",
    "        if 'reviews' in data and not data['reviews']:\n",
    "            break\n",
    "        \n",
    "        page_list = data['reviews'] # Get the list of reviews from the response\n",
    "\n",
    "        for i, page in enumerate(page_list):\n",
    "            words =  page[\"review\"].split() # Split the review text into words\n",
    "\n",
    "            if len(words) < 30: \n",
    "                continue # Skip reviews with less than 30 words\n",
    "            \n",
    "            # Prepare the review text for CSV\n",
    "            rev = '\"' + page[\"review\"].replace('\"',\"\") + '\"'\n",
    "            author_vote = 1 if page['voted_up'] else 0 # Convert voted_up to binary\n",
    "            other_votes = page['votes_up']\n",
    "            weighted_vote_score = page[\"weighted_vote_score\"] \n",
    "            votes_funny = page[\"votes_funny\"] # Get votes_funny or default to 0\n",
    "            comment_count = page[\"comment_count\"] # Get comment_count or default to 0\n",
    "            timestamp_created = datetime.datetime.fromtimestamp(page[\"timestamp_created\"]).strftime('%Y-%m-%d %H:%M:%S') # Convert timestamp to human-readable format\n",
    "            playtime_at_review = page[\"author\"].get(\"playtime_at_review\", 0) #Get playtime_at_review or default to 0\n",
    "\n",
    "            # Write the extracted data to the CSV file\n",
    "            print(rev, ',', author_vote, ',', other_votes, ',', weighted_vote_score, ',', votes_funny, ',', comment_count, ',', timestamp_created, ',', playtime_at_review, file=fo)\n",
    "\n",
    "        # Update the cursor for the next API request\n",
    "        cursor = data['cursor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the SentiWordNet lexicon\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import pandas as pd\n",
    "\n",
    "# Download the SentiWordNet and WordNet resources\n",
    "nltk.download('sentiwordnet');\n",
    "nltk.download('wordnet');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word, pos):\n",
    "    synsets = list(swn.senti_synsets(word, pos))\n",
    "    if not synsets:\n",
    "        return 0, 0, 0\n",
    "    synset = synsets[0]\n",
    "    return synset.pos_score(), synset.neg_score(), synset.obj_score()\n",
    "    \"\"\"\n",
    "    Get the sentiment scores (positive, negative, objective) for a given word and part of speech.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to analyze.\n",
    "        pos (str): The part of speech tag for the word.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Positive score, negative score, and objective score.\n",
    "    \"\"\"\n",
    "\n",
    "def sentiment_analysis_SentiWordNet(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    sentiment_scores = {'positive': 0, 'negative': 0, 'objective': 0 }\n",
    "    word_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    noun_count = 0\n",
    "    adv_count = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a given sentence using SentiWordNet.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Positive score, negative score, objective score, total word count,\n",
    "               count of adjectives, count of verbs, count of nouns, count of adverbs.\n",
    "    \"\"\"\n",
    "\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "\n",
    "        if wn_tag is not None:\n",
    "            #print(f\"{word}={wn_tag}\", end=\", \")\n",
    "            pos_score, neg_score, obj_score = get_sentiment(word, wn_tag)\n",
    "            sentiment_scores['positive'] += pos_score\n",
    "            sentiment_scores['negative'] += neg_score\n",
    "            sentiment_scores['objective'] += obj_score\n",
    "            word_count += 1\n",
    "            # CH: you need to only count for adjectives, verbs, nouns, and adverbs\n",
    "            # if wn_tag is of that type !\n",
    "            if wn_tag == wn.ADJ:\n",
    "                adj_count += 1\n",
    "            if wn_tag == wn.VERB:\n",
    "                verb_count += 1 \n",
    "            if wn_tag == wn.NOUN:\n",
    "                noun_count += 1\n",
    "            if wn_tag == wn.ADV:\n",
    "                adv_count += 1\n",
    "\n",
    "    # Normalize the score by the number of words\n",
    "    if word_count > 0:\n",
    "        sentiment_scores['positive'] /= word_count\n",
    "        sentiment_scores['negative'] /= word_count\n",
    "        sentiment_scores['objective'] /= word_count\n",
    "    return sentiment_scores['positive'], sentiment_scores['negative'], sentiment_scores['objective'], word_count, adj_count, verb_count, noun_count, adv_count\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    \"\"\"\n",
    "    Convert Treebank part of speech tags to WordNet part of speech tags.\n",
    "\n",
    "    Args:\n",
    "        treebank_tag (str): The Treebank part of speech tag.\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding WordNet part of speech tag.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scraped CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(scrape_file, encoding='utf-8'  )\n",
    "print(len(df), \"reviews loaded\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on each review using SentiwordNet\n",
    "# if there is already a results file, skip this and create\n",
    "# an extended file in the next cell\n",
    "results = []\n",
    "\n",
    "reviews = df['review'].tolist()\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    pos_score, neg_score, obj_score, word_count, adj_count, verb_count, noun_count, adv_count = sentiment_analysis_SentiWordNet(review)\n",
    "    results.append({\n",
    "        #'review': review, # CH no need for the reviews after this stage\n",
    "        'positive_score': round(pos_score, 3),\n",
    "        'negative_score': round(neg_score, 3),\n",
    "        'objective_score': round(obj_score, 3),\n",
    "        'word_count': word_count,\n",
    "        'adj_count': adj_count,\n",
    "        'verb_count': verb_count,\n",
    "        'noun_count': noun_count,\n",
    "        'adv_count': adv_count,\n",
    "    })\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=\" \")\n",
    "    \n",
    "# Convert the results to a DataFrame for better readability\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file incase the concat does not work\n",
    "results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(results_file)\n",
    "combined_df = pd.concat([df, results_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we don't have duplicate reviews so drop rows where positive_score, negative_score, objective_score \n",
    "# are the same\n",
    "combined_df = combined_df.drop_duplicates(subset=['positive_score', 'negative_score', 'objective_score'])\n",
    "\n",
    "# re-index the dataframe\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "display(combined_df)\n",
    "\n",
    "# write combined data to a new file\n",
    "# this is so we can load the data back in later without having to re-compute the sentiment analysis\n",
    "combined_df.to_csv(extended_file, index=False, encoding='utf-8')\n",
    "\n",
    "\"\"\"\n",
    "    Remove duplicate rows from the DataFrame based on sentiment scores and save the cleaned DataFrame to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): DataFrame containing combined reviews and sentiment analysis results.\n",
    "        extended_file (str): Path to the CSV file where the cleaned DataFrame will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with duplicates removed and re-indexed.\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
